{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyg69hgNei2y"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense\n",
        "import datetime\n",
        "import pprint\n",
        "import tqdm\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "(train_ds, test_ds), ds_info = tfds.load('cifar10', split=['train', 'test'], as_supervised=True, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_cifar_data(cifar):\n",
        "  #convert data from uint8 to float32\n",
        "  cifar = cifar.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
        "  cifar = cifar.map(lambda img, target: ((img/128.)-1., target))\n",
        "  #create one-hot targets\n",
        "  cifar = cifar.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  cifar = cifar.cache()\n",
        "  #shuffle, batch, prefetch\n",
        "  cifar = cifar.shuffle(1000)\n",
        "  cifar = cifar.batch(32)\n",
        "  cifar = cifar.prefetch(20)\n",
        "  #return preprocessed dataset\n",
        "  return cifar\n",
        "\n",
        "train_ds = train_ds.apply(prepare_cifar_data)\n",
        "test_ds = test_ds.apply(prepare_cifar_data)\n",
        "\n",
        "\n",
        "def try_model(model, ds):\n",
        "  for x, t in ds.take(5):\n",
        "    y = model(x)"
      ],
      "metadata": {
        "id": "CSdALHKle4qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CifarConv(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(CifarConv, self).__init__()\n",
        "\n",
        "        self.convlayer1 = tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu')\n",
        "        self.convlayer2 = tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu')\n",
        "        self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "        self.convlayer3 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu')\n",
        "        self.convlayer4 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu')\n",
        "        self.global_pool = tf.keras.layers.GlobalAvgPool2D()\n",
        "\n",
        "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
        "        tf.keras.metrics.Accuracy(name=\"accuracy\")]\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.convlayer1(x)\n",
        "        x = self.convlayer2(x)\n",
        "        x = self.pooling(x)\n",
        "        x = self.convlayer3(x)\n",
        "        x = self.convlayer4(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "    # 3. metrics property\n",
        "    @property \n",
        "    def metrics(self):\n",
        "      # return a list with all metrics in the model\n",
        "      return self.metrics_list\n",
        "    \n",
        "    # 4. reset all metrics objects\n",
        "    def reset_metrics(self):\n",
        "      for metric in self.metrics:\n",
        "        metric.reset_states()\n",
        "\n",
        "    def train_step(self, data): \n",
        "      img, label = data\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        output = self(img, training=True)\n",
        "        loss = self.loss_function(label, output)\n",
        "\n",
        "      gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    \n",
        "      self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "      # update loss metric\n",
        "      self.metrics[0].update_state(loss)\n",
        "      # for all metrics except loss, update states (accuracy etc.)\n",
        "      for metric in self.metrics[1:]:\n",
        "        metric.update_state(label, output)\n",
        "      return {m.name: m.result() for m in self.metrics}\n",
        "    \n",
        "      # 6. test step\n",
        "    def test_step(self, data):\n",
        "      img, label = data\n",
        "\n",
        "      output = self((img), training=False)\n",
        "      loss = self.loss_function(label, output)\n",
        "    \n",
        "      # update loss metric\n",
        "      self.metrics[0].update_state(loss)\n",
        "      # for all metrics except loss, update states (accuracy etc.)\n",
        "      for metric in self.metrics[1:]:\n",
        "        metric.update_state(label, output)\n",
        "      return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "cifar_model = CifarConv()\n",
        "try_model(cifar_model, train_ds)    "
      ],
      "metadata": {
        "id": "0alFDFwIjFhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define where to save the log\n",
        "config_name= \"config_name\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
        "test_log_path = f\"logs/{config_name}/{current_time}/test\"\n",
        "\n",
        "# log writer for training metrics\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "\n",
        "# log writer for validation metrics\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_path)"
      ],
      "metadata": {
        "id": "rBcSIoKankp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "\n",
        "        # Training: \n",
        "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
        "            #print(data)\n",
        "            metrics = model.train_step(data)\n",
        "\n",
        "            with train_summary_writer.as_default():\n",
        "                for metric in model.metrics:\n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "        \n",
        "        # print the metrics\n",
        "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        # reset all metrics\n",
        "        model.reset_metrics()\n",
        "\n",
        "        # Validation:\n",
        "        for data in test_ds:\n",
        "            metrics = model.test_step(data)\n",
        "\n",
        "            # logging the validation metrics to the log file which is used by tensorboard\n",
        "            with test_summary_writer.as_default():\n",
        "              for metric in model.metrics:\n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "        \n",
        "        print([f\"test_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        # reset all metrics\n",
        "        model.reset_metrics()\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "bJ-g7MJKtwVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model with a meaningful name\n",
        "cifar_model.save_weights(f\"saved_model_{config_name}\", save_format=\"tf\")\n",
        "\n",
        "# load the model:\n",
        "# instantiate a new model from our CNN class\n",
        "#loaded_model = cifar_model(task=2)\n",
        "\n",
        "# load the model weights to continue training. \n",
        "cifar_model.load_weights(f\"saved_model_{config_name}\");\n",
        "\n",
        "# continue training (but: optimizer state is lost)orc barabraians\n",
        "\n",
        "training_loop(model=cifar_model,\n",
        "            train_ds= train_ds,\n",
        "            test_ds= test_ds,\n",
        "            epochs=10,\n",
        "            train_summary_writer=train_summary_writer,\n",
        "            test_summary_writer=test_summary_writer)"
      ],
      "metadata": {
        "id": "lBecbDoinqsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6809322e-94e5-4bf6-c054-bb245e07041e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:37<00:00, 41.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 1.6723462343215942', 'train_accuracy: 0.0']\n",
            "['test_loss: 1.4104536771774292', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:30<00:00, 51.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 1.3179869651794434', 'train_accuracy: 0.0']\n",
            "['test_loss: 1.2165377140045166', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:40<00:00, 38.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 1.1806749105453491', 'train_accuracy: 0.0']\n",
            "['test_loss: 1.160409688949585', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:29<00:00, 52.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 1.1001814603805542', 'train_accuracy: 0.0']\n",
            "['test_loss: 1.0493645668029785', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:31<00:00, 49.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 1.0393104553222656', 'train_accuracy: 0.0']\n",
            "['test_loss: 1.018194317817688', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:30<00:00, 51.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 0.9910697937011719', 'train_accuracy: 0.0']\n",
            "['test_loss: 0.9671459197998047', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:40<00:00, 38.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 0.9524663090705872', 'train_accuracy: 0.0']\n",
            "['test_loss: 0.9557242393493652', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:29<00:00, 52.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 0.9179278016090393', 'train_accuracy: 0.0']\n",
            "['test_loss: 1.0109692811965942', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:31<00:00, 50.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 0.8819910287857056', 'train_accuracy: 0.0']\n",
            "['test_loss: 0.8788188695907593', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:29<00:00, 52.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 0.8545975685119629', 'train_accuracy: 0.0']\n",
            "['test_loss: 0.8962308764457703', 'test_accuracy: 0.0']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-VYMs5CWn1Uh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}